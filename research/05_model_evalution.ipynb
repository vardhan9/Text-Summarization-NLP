{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\MyData\\\\Projects-github\\\\NLP\\\\Text-Summarization-NLP\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\MyData\\\\Projects-github\\\\NLP\\\\Text-Summarization-NLP'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_path = config.model_path,\n",
    "            tokenizer_path = config.tokenizer_path,\n",
    "            metric_file_name = config.metric_file_name\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n",
    "        \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "    \n",
    "    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "            \n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "            \n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "            \n",
    "            # Finally, we decode the generated texts, \n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=True) \n",
    "                for s in summaries]      \n",
    "            \n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            \n",
    "            \n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "            \n",
    "        #  Finally compute and return the ROUGE scores.\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "       \n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        print(dataset_samsum_pt['train'][100:110])\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        \n",
    "        rouge_metric = load_metric('rouge')\n",
    "\n",
    "        score = self.calculate_metric_on_test_ds(\n",
    "        dataset_samsum_pt['train'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    "            )\n",
    "\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index = [f'pegasus'] )\n",
    "        df.to_csv(self.config.metric_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-27 16:27:26,927: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-08-27 16:27:26,934: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-08-27 16:27:26,936: INFO: common: created directory at: artifacts]\n",
      "[2023-08-27 16:27:26,940: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "{'id': ['13682381', '13864991', '13728335', '13727724', '13716510', '13730038', '13865431', '13716842', '13862894', '13811795'], 'dialogue': [\"Gabby: How is you? Settling into the new house OK?\\r\\nSandra: Good. The kids and the rest of the menagerie are doing fine. The dogs absolutely love the new garden. Plenty of room to dig and run around.\\r\\nGabby: What about the hubby?\\r\\nSandra: Well, apart from being his usual grumpy self I guess he's doing OK.\\r\\nGabby: :-D yeah sounds about right for Jim.\\r\\nSandra: He's a man of few words. No surprises there. Give him a backyard shed and that's the last you'll see of him for months.\\r\\nGabby: LOL that describes most men I know.\\r\\nSandra: Ain't that the truth! \\r\\nGabby: Sure is. :-) My one might as well move into the garage. Always tinkering and building something in there.\\r\\nSandra: Ever wondered what he's doing in there?\\r\\nGabby: All the time. But he keeps the place locked.\\r\\nSandra: Prolly building a portable teleporter or something. ;-)\\r\\nGabby: Or a time machine... LOL\\r\\nSandra: Or a new greatly improved Rabbit :-P\\r\\nGabby: I wish... Lmfao!\", 'Nina: Where are you?\\nTim: in the main room\\nAlice: me too, come and join us\\nNina: ok!', 'Esme: Did I tell you what happened last weekend?\\r\\nSummer: Not really.\\r\\nSummer: I mean you mentioned you went to visit Jason and there was a party\\r\\nEsme: Exactly\\r\\nEsme: He was having a party and he didn’t even invite me\\r\\nEsme: How sick is that\\r\\nEsme: The entire band was there\\r\\nEsme: Robert, Tom, Amy and Steve\\r\\nEsme: But he didn’t even bother to invite his girlfriend…\\r\\nSummer: That’s crazy\\r\\nSummer: Why would he do that?\\r\\nEsme: I don’t know.\\r\\nEsme: I got so furious. I broke all of his plates. ', \"Mike: I owe you one!\\r\\nJake: Actually, u owe me 200 :P\\r\\nMike: I'll pay u back as soon as I can.\\r\\nJake: Which is?\\r\\nMike: Well, I should get my paycheck on the 7th, so probably on the 8th.\\r\\nJake: No problem. \\r\\nMike: Thanks again. I don't know what happened to the cash I had set aside for this. \\r\\nJake: Don't worry about it. Last month I had to borrow some money from my parents.\\r\\nMike: Same situation?\\r\\nJake: More or less. Was behind with the rent and landlord became impatient.\\r\\nMike: Straightened things out? \\r\\nJake: Thankfully, yes. \", \"Roger: When you're laying tile how many rows can/should you do in a day?\\r\\nJim: Floor?\\r\\nRoger: walls\\r\\nDan: which mortar are you using?\\r\\nRoger: pre-mixed. seems like it dries very slowly\\r\\nJim: Why would it matter how long it takes to dry? Only takes a day to be able to grout with the pre mix. You can do as many rows as you like also.\\r\\nRoger: Right o. I just didn't know if they would shift around with more rows stacked on them?\\r\\nDan: <file_photo> you just make a level ledger (2nd row). Mark a center line on all walls then see where the last tile is going to be on sides and ceilings. You do the bottom row closest to the shower pan last after the ledger row dries.\\r\\nJim: Exactly, don't do the bottom row 1st, for some reason makes getting everything to line up pretty difficult. And push everything flat on the tile with a level so it looks good.\\r\\nRoger: Okay, good to know. I was just going to start at the bottom and work my way up\\r\\nDan: Might not work. Shower pans are usually not level so everything will end up off.\\r\\nJim: Do you have a digital level? I can give you mine if you need\\r\\nRoger: I have a digital one.\\r\\nJim: Ok cool.\\r\\nDan: Good luck. Let us know if you need anything.\", 'Angelina: wanna go to the cinema to see \"First Man on the Moon\"?\\r\\nJennifer: yes, sure! When?\\r\\nAngelina: I was thinking Friday, like 7-8 pm? Cinema City Arkadia?\\r\\nJennifer: thats ok for me.\\r\\nAngelina: great, see you! ', 'John: Could you buy a set of forks for me at IKEA?\\nJames: LOL, set of forks?\\nJohn: <file_photo> this\\nPatt: no problem!\\nJohn: thanks!', \"Jo: How's your mum, Sue?\\r\\nAnn: Is she gone now?\\r\\nSue: She got back home yesterday\\r\\nSue: The trains were delayed\\r\\nSue: She was furious\\r\\nSue: Still recovering\\r\\nJo: Oh :( \\r\\nAnn: So you're free then... ;)\\r\\nSue: Hurray\\r\\nAnn: A party??\\r\\nSue: I need to recover, too ;)\\r\\nSue: will think about a party later\\r\\nAnn: I take you by the word!\", \"Marek: How was moving out? :c)\\nDaniel: Pretty cool, we moved in about 2 hours, but we're cleaning 2nd day already:D\\nMarek: :d\\nDaniel: I wanna go back to work already xD\\nMarek: Daniel, you ARE at work :d\\nDaniel: omg, you're right :‑O \\nDaniel: :D:D:D\", \"Frances: <file_picture>\\r\\nFrances: Look and guess! Where are we? :D\\r\\nHarold: OMG, is that Strawberry Café?\\r\\nFrances: Yes indeed!\\r\\nHarold: You said you wouldn't have time off?\\r\\nFrances: well one of the meetings got cancelled so we decided to go for a coffee :D\\r\\nHarold: lucky ducky! Have fun then…\\r\\nHarold: <file_gif>\\r\\nFrances: we surely will! Hugs!\"], 'summary': ['Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed.', 'Nina will join Tim and Alice in the main room.', \"Esme is furious about Jason not inviting her to the party. Summer can't understand his behavior.\", 'Mike will repay the money he owes Jake on the 8th. Mike had to pay the rent so he borrowed money from his parents.', 'Roger can do as many rows of tile in a day as he wants. He should make a level ledger, mark a center line and do the bottom row when the ledger row dries.', 'Angelina and Jennifer will go to see \"First Man on the Moon\" on Friday around 7-8 PM at Cinema City Arkadia.', \"James will buy a set of forks at IKEA on John's request.\", \"Sue's mum got back home yesterday after a difficult journey and is still recovering. Sue will think of organising a party later on.\", 'Daniel moved in about two hours but has been cleaning for two days.', 'One of the meetings was cancelled and Frances is at Strawberry Café.'], 'input_ids': [[63327, 151, 722, 117, 119, 152, 110, 84704, 190, 109, 177, 480, 4810, 152, 20387, 151, 1952, 107, 139, 811, 111, 109, 1004, 113, 109, 86409, 127, 557, 1226, 107, 139, 2457, 2068, 298, 109, 177, 1484, 107, 19992, 113, 418, 112, 7358, 111, 550, 279, 107, 63327, 151, 463, 160, 109, 18984, 152, 20387, 151, 1894, 108, 2971, 135, 270, 169, 2985, 42621, 813, 125, 2665, 178, 131, 116, 557, 4810, 107, 63327, 151, 42656, 470, 10036, 2047, 160, 268, 118, 4455, 107, 20387, 151, 285, 131, 116, 114, 729, 113, 324, 989, 107, 566, 7084, 186, 107, 4925, 342, 114, 5869, 5590, 111, 120, 131, 116, 109, 289, 119, 131, 267, 236, 113, 342, 118, 590, 107, 63327, 151, 16013, 120, 5002, 205, 1024, 125, 235, 107, 20387, 151, 44142, 131, 144, 120, 109, 2379, 147, 63327, 151, 7435, 117, 107, 29344, 600, 156, 382, 130, 210, 696, 190, 109, 2733, 107, 6333, 45314, 111, 563, 364, 115, 186, 107, 20387, 151, 7190, 8830, 180, 178, 131, 116, 557, 115, 186, 152, 63327, 151, 436, 109, 166, 107, 343, 178, 3127, 109, 295, 6562, 107, 20387, 151, 1744, 15302, 563, 114, 4361, 42062, 420, 132, 364, 107, 54123, 63327, 151, 1453, 114, 166, 1157, 401, 16013, 20387, 151, 1453, 114, 177, 4125, 2521, 20924, 42656, 969, 63327, 151, 125, 1216, 401, 1054, 36885, 19447, 147, 1], [21430, 151, 3350, 127, 119, 152, 4776, 151, 115, 109, 674, 418, 9399, 151, 213, 314, 108, 331, 111, 1305, 214, 21430, 151, 6514, 147, 1], [15995, 2935, 151, 4162, 125, 823, 119, 180, 2032, 289, 1339, 152, 3835, 151, 1089, 288, 107, 3835, 151, 125, 1021, 119, 2137, 119, 687, 112, 558, 6065, 111, 186, 140, 114, 829, 15995, 2935, 151, 26229, 15995, 2935, 151, 285, 140, 458, 114, 829, 111, 178, 595, 123, 144, 254, 4113, 213, 15995, 2935, 151, 722, 4418, 117, 120, 15995, 2935, 151, 139, 954, 1604, 140, 186, 15995, 2935, 151, 3102, 108, 3227, 108, 8080, 111, 3948, 15995, 2935, 151, 343, 178, 595, 123, 144, 254, 9194, 112, 4113, 169, 10099, 401, 3835, 151, 485, 123, 116, 3628, 3835, 151, 1807, 192, 178, 171, 120, 152, 15995, 2935, 151, 125, 272, 123, 144, 235, 107, 15995, 2935, 151, 125, 419, 167, 28521, 107, 125, 4820, 149, 113, 169, 5838, 107, 1], [3410, 151, 125, 12040, 119, 156, 147, 11472, 151, 10247, 108, 4911, 12040, 213, 2416, 110, 151, 969, 3410, 151, 125, 131, 267, 626, 4911, 247, 130, 783, 130, 125, 137, 107, 11472, 151, 3767, 117, 152, 3410, 151, 1894, 108, 125, 246, 179, 161, 21620, 124, 109, 624, 307, 108, 167, 864, 124, 109, 608, 307, 107, 11472, 151, 566, 575, 107, 3410, 151, 1633, 435, 107, 125, 272, 131, 144, 235, 180, 2032, 112, 109, 1325, 125, 196, 323, 3631, 118, 136, 107, 11472, 151, 1414, 131, 144, 2262, 160, 126, 107, 2882, 625, 125, 196, 112, 10425, 181, 408, 135, 161, 1119, 107, 3410, 151, 10652, 1288, 152, 11472, 151, 1439, 132, 478, 107, 6131, 893, 122, 109, 2813, 111, 10489, 1257, 30576, 107, 3410, 151, 17532, 20797, 341, 165, 152, 11472, 151, 13779, 108, 2816, 107, 1], [10104, 151, 434, 119, 131, 216, 9744, 3724, 199, 223, 9839, 137, 191, 19103, 119, 171, 115, 114, 242, 152, 4455, 151, 6533, 152, 10104, 151, 2622, 4139, 151, 162, 15064, 127, 119, 303, 152, 10104, 151, 1133, 121, 37410, 107, 1045, 172, 126, 20646, 221, 3642, 4455, 151, 1807, 192, 126, 841, 199, 300, 126, 839, 112, 1514, 152, 2786, 839, 114, 242, 112, 129, 350, 112, 15229, 122, 109, 1133, 1707, 107, 226, 137, 171, 130, 223, 9839, 130, 119, 172, 163, 107, 10104, 151, 4002, 4429, 107, 125, 188, 595, 131, 144, 235, 175, 157, 192, 3460, 279, 122, 154, 9839, 14203, 124, 183, 152, 4139, 151, 110, 105, 12014, 940, 18580, 2314, 119, 188, 193, 114, 476, 26598, 5881, 1662, 3843, 250, 2538, 114, 1104, 540, 124, 149, 2622, 237, 236, 241, 109, 289, 3724, 117, 313, 112, 129, 124, 2600, 111, 9298, 107, 226, 171, 109, 1472, 3843, 6682, 112, 109, 2176, 2712, 289, 244, 109, 26598, 3843, 20646, 107, 4455, 151, 26229, 108, 272, 131, 144, 171, 109, 1472, 3843, 305, 1332, 108, 118, 181, 870, 493, 509, 579, 112, 540, 164, 848, 1011, 107, 325, 2589, 579, 2055, 124, 109, 3724, 122, 114, 476, 167, 126, 978, 234, 107, 10104, 151, 15320, 108, 234, 112, 235, 107, 125, 140, 188, 313, 112, 388, 134, 109, 1472, 111, 201, 161, 230, 164, 4139, 151, 21319, 146, 201, 107, 8228, 14871, 127, 832, 146, 476, 167, 579, 138, 370, 164, 299, 107, 4455, 151, 842, 119, 133, 114, 1016, 476, 152, 125, 137, 361, 119, 2643, 175, 119, 217, 10104, 151, 125, 133, 114, 1016, 156, 107, 4455, 151, 10762, 1274, 107, 4139, 151, 1952, 3383, 107, 1593, 214, 235, 175, 119, 217, 742, 107, 1], [44505, 151, 14243, 275, 112, 109, 8840, 112, 236, 198, 3304, 2577, 124, 109, 5275, 24471, 8019, 151, 2816, 108, 334, 147, 434, 152, 44505, 151, 125, 140, 1234, 1197, 108, 172, 36984, 2874, 152, 12609, 672, 15952, 47595, 152, 8019, 151, 120, 116, 6514, 118, 213, 107, 44505, 151, 255, 108, 236, 119, 147, 1], [1084, 151, 7926, 119, 631, 114, 323, 113, 31383, 118, 213, 134, 24713, 152, 2133, 151, 16013, 108, 323, 113, 31383, 152, 1084, 151, 110, 105, 12014, 940, 18580, 2314, 136, 7157, 144, 151, 220, 575, 147, 1084, 151, 1516, 147, 1], [6513, 151, 722, 131, 116, 128, 9996, 108, 12776, 152, 6549, 151, 125, 116, 265, 1871, 239, 152, 12776, 151, 452, 419, 247, 238, 3134, 12776, 151, 139, 7555, 195, 8918, 12776, 151, 452, 140, 28521, 12776, 151, 4587, 11930, 6513, 151, 4384, 43114, 6549, 151, 412, 119, 131, 216, 294, 237, 401, 26408, 12776, 151, 110, 88418, 6549, 151, 202, 829, 6425, 12776, 151, 125, 217, 112, 5097, 108, 314, 26408, 12776, 151, 138, 311, 160, 114, 829, 678, 6549, 151, 125, 248, 119, 141, 109, 1172, 147, 1], [82444, 151, 722, 140, 1218, 165, 152, 110, 151, 1152, 158, 4767, 151, 11167, 1274, 108, 145, 1652, 115, 160, 280, 539, 108, 155, 145, 131, 216, 1496, 280, 1662, 242, 506, 151, 470, 82444, 151, 110, 151, 252, 4767, 151, 125, 14243, 275, 247, 112, 201, 506, 1026, 470, 82444, 151, 4767, 108, 119, 8865, 134, 201, 110, 151, 252, 4767, 151, 25479, 838, 108, 119, 131, 216, 268, 110, 151, 105, 1415, 4767, 151, 110, 151, 470, 151, 470, 151, 470, 1], [26711, 151, 110, 105, 12014, 940, 34584, 2314, 26711, 151, 3842, 111, 2665, 147, 3350, 127, 145, 152, 110, 151, 470, 19386, 151, 32278, 108, 117, 120, 23116, 14534, 152, 26711, 151, 2657, 3482, 147, 19386, 151, 226, 243, 119, 2177, 131, 144, 133, 166, 299, 152, 26711, 151, 210, 156, 113, 109, 2927, 419, 9545, 167, 145, 1159, 112, 275, 118, 114, 1430, 110, 151, 470, 19386, 151, 3434, 10473, 415, 147, 2189, 546, 237, 401, 19386, 151, 110, 105, 12014, 940, 56220, 2314, 26711, 151, 145, 4942, 138, 147, 55013, 147, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[20387, 117, 1374, 190, 109, 177, 480, 206, 215, 328, 117, 774, 122, 126, 107, 1249, 20387, 111, 63327, 1693, 109, 1247, 113, 153, 1024, 111, 6021, 160, 153, 6492, 113, 2027, 166, 115, 109, 2733, 132, 114, 5590, 107, 1], [21430, 138, 1305, 4776, 111, 9399, 115, 109, 674, 418, 107, 1], [15995, 2935, 117, 28521, 160, 6065, 146, 7426, 215, 112, 109, 829, 107, 3835, 137, 131, 144, 630, 169, 2764, 107, 1], [3410, 138, 17576, 109, 408, 178, 26708, 11472, 124, 109, 608, 307, 107, 3410, 196, 112, 626, 109, 2813, 167, 178, 15278, 408, 135, 169, 1119, 107, 1], [10104, 137, 171, 130, 223, 9839, 113, 3724, 115, 114, 242, 130, 178, 1728, 107, 285, 246, 193, 114, 476, 26598, 108, 2689, 114, 1104, 540, 111, 171, 109, 1472, 3843, 173, 109, 26598, 3843, 20646, 107, 1], [44505, 111, 8019, 138, 275, 112, 236, 198, 3304, 2577, 124, 109, 5275, 194, 124, 1197, 279, 36984, 3031, 134, 12609, 672, 15952, 47595, 107, 1], [2133, 138, 631, 114, 323, 113, 31383, 134, 24713, 124, 1084, 131, 116, 1320, 107, 1], [12776, 131, 116, 9996, 419, 247, 238, 3134, 244, 114, 1011, 1486, 111, 117, 309, 11930, 107, 12776, 138, 311, 113, 16588, 114, 829, 678, 124, 107, 1], [4767, 1652, 115, 160, 228, 539, 155, 148, 174, 1496, 118, 228, 390, 107, 1], [614, 113, 109, 2927, 140, 9545, 111, 26711, 117, 134, 23116, 14534, 107, 1]]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2445aa00f070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel_evaluation_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-2445aa00f070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel_evaluation_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model_evaluation_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_evaluation_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_evaluation_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel_evaluation_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-08475bcfbfaf>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mrouge_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"rouge1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rouge2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rougeL\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rougeLsum\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mrouge_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rouge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         score = self.calculate_metric_on_test_ds(\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_metric\u001b[1;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **metric_init_kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m     ).module_path\n\u001b[1;32m-> 1392\u001b[1;33m     \u001b[0mmetric_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_main_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m     metric = metric_cls(\n\u001b[0;32m   1394\u001b[0m         \u001b[0mconfig_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mimport_main_class\u001b[1;34m(module_path, dataset)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetric\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\rouge\\0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f\\rouge.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m  \u001b[1;31m# Here to have a nice missing dependency error message early on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m  \u001b[1;31m# Here to have a nice missing dependency error message early on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\site-packages\\rouge_score\\rouge_scorer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mRougeScorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseScorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m   \"\"\"Calculate rouges scores between two blobs of text.\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MyData\\Fliq\\installations\\AI\\envs\\datascience\\lib\\site-packages\\rouge_score\\rouge_scorer.py\u001b[0m in \u001b[0;36mRougeScorer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_summaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0mscore_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \"\"\"Calculates rouge scores between targets and prediction.\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation_config.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
