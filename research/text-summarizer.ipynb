{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-24T09:04:30.867603Z","iopub.execute_input":"2023-08-24T09:04:30.868014Z","iopub.status.idle":"2023-08-24T09:04:30.875594Z","shell.execute_reply.started":"2023-08-24T09:04:30.867983Z","shell.execute_reply":"2023-08-24T09:04:30.874355Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:04:38.326708Z","iopub.execute_input":"2023-08-24T09:04:38.327077Z","iopub.status.idle":"2023-08-24T09:04:39.484303Z","shell.execute_reply.started":"2023-08-24T09:04:38.327048Z","shell.execute_reply":"2023-08-24T09:04:39.483022Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Thu Aug 24 09:04:39 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:50:35.294024Z","iopub.execute_input":"2023-08-24T08:50:35.294391Z","iopub.status.idle":"2023-08-24T08:50:47.348841Z","shell.execute_reply.started":"2023-08-24T08:50:35.294360Z","shell.execute_reply":"2023-08-24T08:50:47.347566Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"! pip install datasets","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:04:43.220913Z","iopub.execute_input":"2023-08-24T09:04:43.222213Z","iopub.status.idle":"2023-08-24T09:04:55.069180Z","shell.execute_reply.started":"2023-08-24T09:04:43.222154Z","shell.execute_reply":"2023-08-24T09:04:55.067780Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:15:08.264960Z","iopub.execute_input":"2023-08-24T09:15:08.265309Z","iopub.status.idle":"2023-08-24T09:15:08.270064Z","shell.execute_reply.started":"2023-08-24T09:15:08.265280Z","shell.execute_reply":"2023-08-24T09:15:08.268948Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade accelerate\n!pip uninstall -y transformers accelerate\n!pip install transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:51:59.116190Z","iopub.execute_input":"2023-08-24T08:51:59.116575Z","iopub.status.idle":"2023-08-24T08:52:35.487815Z","shell.execute_reply.started":"2023-08-24T08:51:59.116539Z","shell.execute_reply":"2023-08-24T08:52:35.486559Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting accelerate\n  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.22.0\nFound existing installation: transformers 4.30.2\nUninstalling transformers-4.30.2:\n  Successfully uninstalled transformers-4.30.2\nFound existing installation: accelerate 0.22.0\nUninstalling accelerate-0.22.0:\n  Successfully uninstalled accelerate-0.22.0\nCollecting transformers\n  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting accelerate\n  Using cached accelerate-0.22.0-py3-none-any.whl (251 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: transformers, accelerate\nSuccessfully installed accelerate-0.22.0 transformers-4.32.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom datasets import load_dataset, load_from_disk\nimport matplotlib.pyplot  as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom tqdm import tqdm\nimport torch\nnltk.download(\"punkt\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:15:54.652670Z","iopub.execute_input":"2023-08-24T09:15:54.653061Z","iopub.status.idle":"2023-08-24T09:15:54.834292Z","shell.execute_reply.started":"2023-08-24T09:15:54.653027Z","shell.execute_reply":"2023-08-24T09:15:54.833364Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:55:28.300112Z","iopub.execute_input":"2023-08-24T09:55:28.300495Z","iopub.status.idle":"2023-08-24T09:56:05.125592Z","shell.execute_reply.started":"2023-08-24T09:55:28.300457Z","shell.execute_reply":"2023-08-24T09:56:05.124566Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442312c278084e0c8ac006733a817187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f976781ac2463db256b4188ae2ed34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd0dcab8a704835907298fac5901c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d3a894774045d6a3f0bba5e4b55014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb0ded9bc954f2a86c95a01fa8b8269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e153c8c3e469437c875558e7c2be7f0d"}},"metadata":{}}]},{"cell_type":"code","source":"#dowload & unzip data\n\n!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n!unzip summarizer-data.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:57:19.094343Z","iopub.execute_input":"2023-08-24T09:57:19.095038Z","iopub.status.idle":"2023-08-24T09:57:22.583712Z","shell.execute_reply.started":"2023-08-24T09:57:19.095005Z","shell.execute_reply":"2023-08-24T09:57:22.582527Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"--2023-08-24 09:57:20--  https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip [following]\n--2023-08-24 09:57:20--  https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7903594 (7.5M) [application/zip]\nSaving to: ‘summarizer-data.zip’\n\nsummarizer-data.zip 100%[===================>]   7.54M  --.-KB/s    in 0.08s   \n\n2023-08-24 09:57:20 (89.1 MB/s) - ‘summarizer-data.zip’ saved [7903594/7903594]\n\nArchive:  summarizer-data.zip\n  inflating: samsum-test.csv         \n  inflating: samsum-train.csv        \n  inflating: samsum-validation.csv   \n   creating: samsum_dataset/\n extracting: samsum_dataset/dataset_dict.json  \n   creating: samsum_dataset/test/\n  inflating: samsum_dataset/test/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/test/dataset_info.json  \n  inflating: samsum_dataset/test/state.json  \n   creating: samsum_dataset/train/\n  inflating: samsum_dataset/train/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/train/dataset_info.json  \n  inflating: samsum_dataset/train/state.json  \n   creating: samsum_dataset/validation/\n  inflating: samsum_dataset/validation/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/validation/dataset_info.json  \n  inflating: samsum_dataset/validation/state.json  \n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_samsum = load_from_disk('samsum_dataset')\ndataset_samsum","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:58:38.327292Z","iopub.execute_input":"2023-08-24T09:58:38.328326Z","iopub.status.idle":"2023-08-24T09:58:38.369473Z","shell.execute_reply.started":"2023-08-24T09:58:38.328267Z","shell.execute_reply":"2023-08-24T09:58:38.368466Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\nprint(\"\\nDialogue:\")\n\nprint(dataset_samsum[\"test\"][1][\"dialogue\"])\n\nprint(\"\\nSummary:\")\n\nprint(dataset_samsum[\"test\"][1][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:58:53.241907Z","iopub.execute_input":"2023-08-24T09:58:53.242377Z","iopub.status.idle":"2023-08-24T09:58:53.254394Z","shell.execute_reply.started":"2023-08-24T09:58:53.242347Z","shell.execute_reply":"2023-08-24T09:58:53.253495Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Split lengths: [14732, 819, 818]\nFeatures: ['id', 'dialogue', 'summary']\n\nDialogue:\nEric: MACHINE!\nRob: That's so gr8!\nEric: I know! And shows how Americans see Russian ;)\nRob: And it's really funny!\nEric: I know! I especially like the train part!\nRob: Hahaha! No one talks to the machine like that!\nEric: Is this his only stand-up?\nRob: Idk. I'll check.\nEric: Sure.\nRob: Turns out no! There are some of his stand-ups on youtube.\nEric: Gr8! I'll watch them now!\nRob: Me too!\nEric: MACHINE!\nRob: MACHINE!\nEric: TTYL?\nRob: Sure :)\n\nSummary:\nEric and Rob are going to watch a stand-up on youtube.\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n    \n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n        \n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:59:18.566523Z","iopub.execute_input":"2023-08-24T09:59:18.567489Z","iopub.status.idle":"2023-08-24T09:59:18.574273Z","shell.execute_reply.started":"2023-08-24T09:59:18.567429Z","shell.execute_reply":"2023-08-24T09:59:18.572967Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T09:59:29.977463Z","iopub.execute_input":"2023-08-24T09:59:29.977863Z","iopub.status.idle":"2023-08-24T09:59:37.825636Z","shell.execute_reply.started":"2023-08-24T09:59:29.977833Z","shell.execute_reply":"2023-08-24T09:59:37.824428Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f7f4a86e93f4cf0b2808053db880456"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb97a1deef842a2aa7ec35168238a33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737c90893d9744b7be69d9ff67473480"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum_pt","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:00:02.979284Z","iopub.execute_input":"2023-08-24T10:00:02.980415Z","iopub.status.idle":"2023-08-24T10:00:02.987712Z","shell.execute_reply.started":"2023-08-24T10:00:02.980377Z","shell.execute_reply":"2023-08-24T10:00:02.986787Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Training\n\nfrom transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:00:28.039476Z","iopub.execute_input":"2023-08-24T10:00:28.039965Z","iopub.status.idle":"2023-08-24T10:00:28.048560Z","shell.execute_reply.started":"2023-08-24T10:00:28.039925Z","shell.execute_reply":"2023-08-24T10:00:28.045649Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntrainer_args = TrainingArguments(\n    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n    weight_decay=0.01, logging_steps=10,\n    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16\n) ","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:00:40.635053Z","iopub.execute_input":"2023-08-24T10:00:40.635422Z","iopub.status.idle":"2023-08-24T10:00:40.669015Z","shell.execute_reply.started":"2023-08-24T10:00:40.635386Z","shell.execute_reply":"2023-08-24T10:00:40.668084Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\ntrainer = Trainer(model=model_pegasus, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"test\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:00:51.878892Z","iopub.execute_input":"2023-08-24T10:00:51.880032Z","iopub.status.idle":"2023-08-24T10:00:51.901630Z","shell.execute_reply.started":"2023-08-24T10:00:51.879998Z","shell.execute_reply":"2023-08-24T10:00:51.900707Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:01:05.459941Z","iopub.execute_input":"2023-08-24T10:01:05.460314Z","iopub.status.idle":"2023-08-24T10:10:21.051546Z","shell.execute_reply.started":"2023-08-24T10:01:05.460284Z","shell.execute_reply":"2023-08-24T10:10:21.050485Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230824_100416-btppsdfz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/marthalarajavardhanreddy/huggingface/runs/btppsdfz' target=\"_blank\">fearless-mountain-1</a></strong> to <a href='https://wandb.ai/marthalarajavardhanreddy/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/marthalarajavardhanreddy/huggingface' target=\"_blank\">https://wandb.ai/marthalarajavardhanreddy/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/marthalarajavardhanreddy/huggingface/runs/btppsdfz' target=\"_blank\">https://wandb.ai/marthalarajavardhanreddy/huggingface/runs/btppsdfz</a>"},"metadata":{}},{"name":"stderr","text":"You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 05:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=25, training_loss=3.0184992599487304, metrics={'train_runtime': 554.8717, 'train_samples_per_second': 1.476, 'train_steps_per_second': 0.045, 'total_flos': 420987064270848.0, 'train_loss': 3.0184992599487304, 'epoch': 0.98})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluation\n\ndef generate_batch_sized_chunks(list_of_elements, batch_size):\n    \"\"\"split the dataset into smaller batches that we can process simultaneously\n    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\n\n\ndef calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n                               batch_size=16, device=device, \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n        \n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                        padding=\"max_length\", return_tensors=\"pt\")\n        \n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                         attention_mask=inputs[\"attention_mask\"].to(device), \n                         length_penalty=0.8, num_beams=8, max_length=128)\n        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n        \n        # Finally, we decode the generated texts, \n        # replace the  token, and add the decoded texts with the references to the metric.\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                clean_up_tokenization_spaces=True) \n               for s in summaries]      \n        \n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n        \n        \n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n        \n    #  Finally compute and return the ROUGE scores.\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:11:58.496555Z","iopub.execute_input":"2023-08-24T10:11:58.499287Z","iopub.status.idle":"2023-08-24T10:11:58.520654Z","shell.execute_reply.started":"2023-08-24T10:11:58.499241Z","shell.execute_reply":"2023-08-24T10:11:58.519375Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:13:03.960432Z","iopub.execute_input":"2023-08-24T10:13:03.961528Z","iopub.status.idle":"2023-08-24T10:13:17.992807Z","shell.execute_reply.started":"2023-08-24T10:13:03.961482Z","shell.execute_reply":"2023-08-24T10:13:17.991537Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=91636f1cd405d9b6b2f2860f9556d3fbd09962629b8ef34b534b478f3e4f4c05\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_metric = load_metric('rouge')","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:13:27.868391Z","iopub.execute_input":"2023-08-24T10:13:27.869361Z","iopub.status.idle":"2023-08-24T10:13:28.120525Z","shell.execute_reply.started":"2023-08-24T10:13:27.869306Z","shell.execute_reply":"2023-08-24T10:13:28.119146Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"score = calculate_metric_on_test_ds(\n    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n)\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\npd.DataFrame(rouge_dict, index = [f'pegasus'] )","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:15:01.486683Z","iopub.execute_input":"2023-08-24T10:15:01.487066Z","iopub.status.idle":"2023-08-24T10:15:19.072988Z","shell.execute_reply.started":"2023-08-24T10:15:01.487029Z","shell.execute_reply":"2023-08-24T10:15:19.071804Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|██████████| 5/5 [00:17<00:00,  3.45s/it]\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"           rouge1  rouge2    rougeL  rougeLsum\npegasus  0.020865     0.0  0.020828    0.02044","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.020865</td>\n      <td>0.0</td>\n      <td>0.020828</td>\n      <td>0.02044</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Save model\nmodel_pegasus.save_pretrained(\"pegasus-samsum-model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:16:06.141679Z","iopub.execute_input":"2023-08-24T10:16:06.142074Z","iopub.status.idle":"2023-08-24T10:16:12.392627Z","shell.execute_reply.started":"2023-08-24T10:16:06.142044Z","shell.execute_reply":"2023-08-24T10:16:12.390932Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"## Save tokenizer\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:16:42.601570Z","iopub.execute_input":"2023-08-24T10:16:42.602556Z","iopub.status.idle":"2023-08-24T10:16:42.817649Z","shell.execute_reply.started":"2023-08-24T10:16:42.602512Z","shell.execute_reply":"2023-08-24T10:16:42.816460Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/spiece.model',\n 'tokenizer/added_tokens.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"#Load\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:17:36.964934Z","iopub.execute_input":"2023-08-24T10:17:36.965296Z","iopub.status.idle":"2023-08-24T10:17:37.154097Z","shell.execute_reply.started":"2023-08-24T10:17:36.965268Z","shell.execute_reply":"2023-08-24T10:17:37.153007Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Prediction\n\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n\n\n\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n\npipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n\n## \nprint(\"Dialogue:\")\nprint(sample_text)\n\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-24T10:18:05.598132Z","iopub.execute_input":"2023-08-24T10:18:05.598538Z","iopub.status.idle":"2023-08-24T10:18:47.020661Z","shell.execute_reply.started":"2023-08-24T10:18:05.598506Z","shell.execute_reply":"2023-08-24T10:18:47.019633Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}